# Design Document for Custom Langchain Agent

## Problem Statement

Given 2 input CSV files and 2 input text files, create a langchain agent capable of handling Q&A in context of those files.
The 4 files will be related and the agent must able to find information across both text and csv files.


## High level Design

### Data PreProcessing

Once the user inputs the file, I prompted Open AI to get more information related to the file, such as the file description which will be used as input for the langchain agents created for each file. The format of the description is as follows:
'Useful for when you need to answer questions about (enter description here along with explanantion of columns headers with some example values). Input should be a fully formed question.'"

I also got information from gpt-4 related to whether the contents of the a given text file are well structured and can be easily converted to a csv format. If an input text file contains structured data, it is converted to a csv file used as input for a csv agent, since its able parse a csv file much easier with high accuracy as compared to a text file.

### Create Tools and initialize Master Agent

After the file data have been pre processed, I create an agent for each file. 
For text files, a text chain is created which first splits the text data into chunks and creates open AI embeddings to be used as input for a Retrieval Question/Answering chain. This chain is capable of handling questions pertaining to the content in this file.
For CSV files, a csv agent is created using the csv file as input.

Now for each of these 4 agents, I create a corresponding "Tool" object. Each tool object takes the file name, file description(generated by GPT-4) and the function associated with the file, which in this case, will be the agent created specifically for that file(either csv agent or text chain). 

All the 4 Tool objects are combined in a list and passed in as a parameter to initialize a master agent. The type of this agent is specified to be "zero-shot-react-description", which means that this agent will use the ReAct framework to determine which tool to use based solely on the tool's description. This type of agent supports any number of tools as long as they have a clear description associated with it. To learn more agent types, check out https://python.langchain.com/en/latest/modules/agents/agents/agent_types.html.

### Run Queries

After the master agent has been initialized, you can now prompt it with queries related to deriving information from one or more input files.
An example query looks like the following: 
"What is the syllabus for the subject taught in 3rd period on Monday?"
Then agent enters an `AgentExecutor` chain using this query. In this chain, the agent will figure out which tool to use to derive what subject is taught in 3rd period on Monday. In this example, the agent uses the `Weekly Schedule` tool and makes a query to find the corresponding answer("Chemistry" in this example). After an answer is returned by the `AgentExecutor`, the chain is completed and now the agent has to find the syllabus for this subject. So it enters a new `AgentExecutor` chain which goes to the `Syllabus` tool to find the syllabus for the corresponding subject and returns the final answer of the finished chain. This process repeats till there are no more `AgentExecutor` chains created.


## Interesting findings

The agent performs really well when the input data is structured and converted to csv format. In an earlier iteration of the project, I was inputting the text file as is to the agent and it sometimes had issues deriving information about the weekly schedule since its unable to make any programattic queries for finding the answer. Instead, it has to rely on the vector store and find chunks of text which can relate to the prompt entered by the user, in order to derive an answer. In this case, another important parameter, is the chunk size used to split the text, since we want the related text content to be in a single chunk so the agent can easily derive information from it. For structured text files(which are converted to CSVs in the pre processing step), the agent was able to provide consistently accurate answers to the queries.

Another issue I noticed with this multi tool agent, was how it's redirecting the question to a tool based on the contents of the question. For example, consider this prompt:
"Which of the Mathematics professors are male?
The agent goes to the `Professors` tool to find information related to the demographics and subjects taught by the professors.
It runs the following action to filter for professors that teach Mathematics:
`df[df['education'] == 'Ph.D.']`
The agent jumps to the wrong conclusion that subject information is present in the `education` column of the `Professors` file and returns an incorrect answer. 
After experimenting with the agent inputs and trying more detailed descriptions which include column descriptions with examples, the agent still makes the same mistake. It is looking for information that is not contained in `Professors` tool.

To work around this problem, I tried experiementing with the prompt itself and found better results;
In the following prompt:
"Which of the male professors teach Mathematics in the weekly schedule?"
The agent has more information about what piece of information is contained in which tool so it's able to get the correct answer. I noticed that it's important for the user to have some context about the input files and what information they contain, to prevent the agent from making wrong conclusions about what may or may not be in a file. Keeping that in mind, constructing a prompt which groups queries with the corresponding tool name really helpes the agent think clearly and redirect itself to the correct tool to derive what the user is looking for.
